{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Embeddings"
    ]
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "\n",
    "This code example provides a contextualization mechanism that retrieves relevant context entries based on an input vector. It uses a combination of techniques to achieve a balance between sophistication, effectiveness, and ease of set-up and implementation.\n",
    "Vector Database: The code utilizes a vector database (ChromaDB) to store and retrieve contextual information efficiently.\n",
    "Similarity Search: The retrieve_context function employs a similarity search algorithm (cosine similarity) to find relevant context entries.\n",
    "Weighted Sum: The function calculates a weighted sum of relevant context vectors to create a single context vector.\n",
    "Time-Aware: The code incorporates a time-aware component to prioritize more recent context entries.\n",
    "The store_context function allows you to store context entries in the vector database, which can be retrieved later using the retrieve_context function.\n",
    "\n",
    "Here's a more detailed, granular implementation of the interactive functionality:\n",
    "1. Step 1: User Input Processing\n",
    "Receive user input through the interface (e.g., text, voice, or other modalities)\n",
    "Preprocess the input text (e.g., tokenization, stopword removal, stemming)\n",
    "Convert the input text into a dense vector representation using a language model (e.g., BERT, RoBERTa)\n",
    "2. Step 2: Contextualization\n",
    "Use the vector database to retrieve relevant chat history, context, and knowledge base entries based on the user's input vector\n",
    "Implement a similarity search algorithm (e.g., cosine similarity, dot product) to find the most relevant entries\n",
    "Retrieve the top-N most similar entries and concatenate them into a single context vector\n",
    "Step 3: LLM Input Preparation\n",
    "Concatenate the user input vector with the context vector to create a single input vector for the LLM\n",
    "Optionally, add additional metadata (e.g., user ID, conversation ID, timestamp) to the input vector\n",
    "Step 4: LLM Response Generation\n",
    "Send the input vector to the LLM and receive a generated response\n",
    "Postprocess the response (e.g., spell checking, grammar correction)\n",
    "Step 5: Response Retrieval and Ranking\n",
    "Use the vector database to retrieve relevant knowledge base entries based on the LLM's response vector\n",
    "Implement a ranking algorithm (e.g., relevance scoring, diversity-based ranking) to select the top-N most relevant entries\n",
    "Return the ranked list of knowledge base entries to the user\n",
    "Step 6: Feedback Loop\n",
    "Store the user's input, LLM response, and retrieved knowledge base entries in the vector database\n",
    "Use this data to improve the contextualization mechanism, LLM response generation, and knowledge base retrieval over time\n",
    "Example Code Snippets\n",
    "Here are some example code snippets to demonstrate the granular implementation:\n",
    "User Input Processing\n",
    "Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a function to process user input\n",
    "def process_input(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    input_vector = outputs.last_hidden_state[:, 0, :]\n",
    "    return input_vector\n",
    "Contextualization\n",
    "Python\n",
    "import chromadb\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Define a function to retrieve relevant context\n",
    "def retrieve_context(input_vector):\n",
    "    # Create a query vector from the input vector\n",
    "    query_vector = input_vector.numpy()\n",
    "\n",
    "    # Retrieve top-N most similar context entries\n",
    "    results = client.query({\n",
    "        \"vector\": query_vector.tolist(),\n",
    "        \"top_k\": 5\n",
    "    })\n",
    "\n",
    "    # Concatenate context vectors into a single vector\n",
    "    context_vector = np.concatenate([result[\"vector\"] for result in results])\n",
    "    return context_vector\n",
    "LLM Response Generation\n",
    "Python\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load pre-trained LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Define a function to generate LLM response\n",
    "def generate_response(input_vector):\n",
    "    # Convert input vector to a tensor\n",
    "    input_tensor = torch.tensor(input_vector)\n",
    "\n",
    "    # Generate response using the LLM\n",
    "    outputs = model.generate(input_tensor)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "Response Retrieval and Ranking\n",
    "Python\n",
    "import chromadb\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Define a function to retrieve relevant knowledge base entries\n",
    "def retrieve_knowledge_base(response_vector):\n",
    "    # Create a query vector from the response vector\n",
    "    query_vector = response_vector.numpy()\n",
    "\n",
    "    # Retrieve top-N most similar knowledge base entries\n",
    "    results = client.query({\n",
    "        \"vector\": query_vector.tolist(),\n",
    "        \"top_k\": 5\n",
    "    })\n",
    "\n",
    "    # Rank knowledge base entries based on relevance\n",
    "    ranked_results = sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "    return ranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import random\n",
    "\n",
    "# Define constants for probability values\n",
    "GENERATE_RANDOM_PROMPT_CHANCE = 0.1\n",
    "GENERATE_SATANIC_RESPONSE_CHANCE = 0.05\n",
    "GENERATE_NEW_PROMPT_CHANCE = 0.01\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=\"auto\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "        self.chat_history: list[str] = []\n",
    "        self.creative_projects_pool: list[str] = [\n",
    "            \"design a detailed framework for a futuristic sustainable city\",\n",
    "            \"create a concept for an app that tracks and manages mental health\",\n",
    "            \"develop a game where players solve puzzles to progress through levels\",\n",
    "            \"write a script for a short animated film about self-discovery\",\n",
    "            \"compose music for a short film about alien invasion\",\n",
    "            \"invent a new material that can be used in various industries\",\n",
    "            \"design a futuristic transportation system\",\n",
    "            \"create a new sport that combines elements of existing sports\",\n",
    "            \"develop an AI-powered system to help with everyday tasks\"\n",
    "        ]\n",
    "        self.creative_premise_pool: list[str] = [\n",
    "            \"in a world where magic has returned after centuries\",\n",
    "            \"where the boundaries between reality and virtual reality have blurred\",\n",
    "            \"on a planet on the brink of environmental disaster\",\n",
    "            \"in a society where technology has surpassed human intelligence\",\n",
    "            \"as the last remnants of humanity fight to survive in a post-apocalyptic world\"\n",
    "        ]\n",
    "        self.character_pools = {\n",
    "            \"god\": [\n",
    "                \"a being of immense power and wisdom\",\n",
    "                \"a creator who seeks to understand the mysteries of the universe\",\n",
    "                \"an entity with the ability to shape reality itself\"\n",
    "            ],\n",
    "            \"satan\": [\n",
    "                \"the embodiment of chaos and destruction\",\n",
    "                \"a being of immense cunning and deception\",\n",
    "                \"the master of darkness and shadow\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def generate_random_prompts_and_responses(self):\n",
    "        \"\"\"Generate random prompts and responses based on given probabilities.\"\"\"\n",
    "        if random.random() < GENERATE_RANDOM_PROMPT_CHANCE:\n",
    "            # Randomly select a creative premise idea\n",
    "            premise = random.choice(self.creative_premise_pool)\n",
    "\n",
    "            # Randomly select a creative project idea\n",
    "            project = random.choice(self.creative_projects_pool)\n",
    "\n",
    "            # Append the selected premise and project together to chat history\n",
    "            prompt = f\"{premise}{project}\"\n",
    "            self.chat_history.append(prompt)\n",
    "\n",
    "            # Tokenize input text and generate response using causal language model\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "            output = self.model.generate(**inputs, max_length=100)\n",
    "            decoded_response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            character = random.choice(list(self.character_pools.keys()))\n",
    "            self.chat_history.append(f\"{character}: {decoded_response}\")\n",
    "\n",
    "        if random.random() < GENERATE_SATANIC_RESPONSE_CHANCE:\n",
    "            # Randomly select a creative premise idea\n",
    "            premise = random.choice(self.creative_premise_pool)\n",
    "\n",
    "            # Randomly select a creative project idea\n",
    "            project = random.choice(self.creative_projects_pool)\n",
    "\n",
    "            # Append the selected premise and project together to chat history\n",
    "            prompt = f\"{premise}{project}\"\n",
    "            self.chat_history.append(prompt)\n",
    "\n",
    "            # Tokenize input text and generate response using causal language model\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "            output = self.model.generate(**inputs, max_length=100)\n",
    "            decoded_response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            character = random.choice(list(self.character_pools.keys()))\n",
    "            if character == \"satan\":\n",
    "                self.chat_history.append(f\"{character}: {decoded_response}\")\n",
    "            else:\n",
    "                # Randomly select a creative premise idea to respond to satanic prompt\n",
    "                satanic_premise = random.choice(self.creative_premise_pool)\n",
    "\n",
    "                # Tokenize input text and generate response using causal language model\n",
    "                inputs = self.tokenizer(satanic_premise, return_tensors=\"pt\")\n",
    "                output = self.model.generate(**inputs, max_length=100)\n",
    "                decoded_response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "                self.chat_history.append(f\"{character}: {decoded_response}\")\n",
    "\n",
    "        if random.random() < GENERATE_NEW_PROMPT_CHANCE:\n",
    "            # Randomly select a creative premise idea\n",
    "            premise = random.choice(self.creative_premise_pool)\n",
    "\n",
    "            # Append the selected premise to chat history\n",
    "            self.chat_history.append(premise)\n",
    "\n",
    "    def converse(self):\n",
    "        \"\"\"Converse indefinitely.\"\"\"\n",
    "        while True:\n",
    "            time.sleep(random.uniform(0.5, 2))\n",
    "            self.generate_random_prompts_and_responses()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = ChatBot()\n",
    "    chatbot.converse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin Groups of Modifiers, grouped by type or style of task and content being worked on.\n",
    "# On run, modifiers are appended to the end of the latest ai response and fed back to the AI to either critique or refine its previous response.\n",
    "\n",
    "# This modifier group is generalized ideation intialization\n",
    "from collections import deque\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from search_manager import GOOGLE_CUSTOM_SEARCH_API_KEY, GOOGLE_CUSTOM_SEARCH_ENGINE_ID\n",
    "# Constants\n",
    "MAX_CHAT_HISTORY_LENGTH = 14\n",
    "MAX_RETRIES = 3\n",
    "BACKOFF_FACTOR = 2\n",
    "# Constants\n",
    "\n",
    "load_dotenv()\n",
    "#LLM\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "#SEARCH APIS\n",
    "#GOOGLE_CUSTOM_SEARCH_API_KEY = os.getenv('GOOGLE_CUSTOM_SEARCH_API_KEY')\n",
    "#GOOGLE_CUSTOM_SEARCH_ENGINE_ID = os.getenv('GOOGLE_CUSTOM_SEARCH_ENGINE_ID')\n",
    "BRAVE_SEARCH_API_KEY = os.getenv('BRAVE_SEARCH_API_KEY')\n",
    "\n",
    "# Constants\n",
    "MAX_TRUNCATE_LENGTH = 5000 #Length of truncated chat_log items\n",
    "MAX_INVALID_ATTEMPTS = 3\n",
    "MAX_SEARCH_RESULTS = 8\n",
    "MAX_CONTENT_LENGTH = 4000  # Maximum number of characters to extract from each webpage\n",
    "TIMEOUT = 15\n",
    "MAX_SEARCH_QUERIES_PER_REQUEST = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gui.py\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, scrolledtext, Menu, filedialog, simpledialog, messagebox\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import List, Optional\n",
    "from functools import partial\n",
    "\n",
    "from models import ModelManager, generate_convo_context\n",
    "from agents import AgentManager\n",
    "from search_manager import SearchManager\n",
    "from config import MAX_SEARCH_RESULTS\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ModifierTreeView(ttk.Treeview):\n",
    "    def __init__(self, master, **kwargs):\n",
    "        super().__init__(master, **kwargs)\n",
    "        self.selected_modifiers: List[str] = []\n",
    "        self.bind(\"<<TreeviewSelect>>\", self.on_select)\n",
    "\n",
    "    def on_select(self, event):\n",
    "        selected_item = self.selection()[0]\n",
    "        item_text = self.item(selected_item, \"text\")\n",
    "        if item_text not in self.selected_modifiers:\n",
    "            self.selected_modifiers.append(item_text)\n",
    "\n",
    "class App(tk.Tk):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.title(\"Creative AI Assistant\")\n",
    "        self.geometry(\"800x600\")\n",
    "\n",
    "        self.model_manager = ModelManager(search_enabled=True)\n",
    "        self.agent_manager = AgentManager()\n",
    "        self.search_manager = SearchManager()\n",
    "\n",
    "        self.chat_log: List[str] = []\n",
    "        self.context = \"\"\n",
    "        self.current_prompt = \"\"\n",
    "        self.last_output = \"\"\n",
    "\n",
    "        self.setup_ui()\n",
    "        self.setup_menu()\n",
    "\n",
    "    def setup_ui(self):\n",
    "        self.main_frame = ttk.Frame(self)\n",
    "        self.main_frame.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "        self.sidebar_frame = ttk.Frame(self)\n",
    "        self.sidebar_frame.pack(side=\"right\", fill=\"y\")\n",
    "\n",
    "        self.chat_history = scrolledtext.ScrolledText(self.main_frame, wrap=\"word\")\n",
    "        self.chat_history.pack(fill=\"both\", expand=True)\n",
    "\n",
    "        self.user_input = ttk.Entry(self.main_frame)\n",
    "        self.user_input.pack(fill=\"x\")\n",
    "        self.user_input.bind(\"<Return>\", self.run_workflow)\n",
    "\n",
    "        self.setup_sidebar()\n",
    "\n",
    "    def setup_sidebar(self):\n",
    "        self.model_label = ttk.Label(self.sidebar_frame, text=\"Model:\")\n",
    "        self.model_label.pack()\n",
    "\n",
    "        self.model_var = tk.StringVar(value=\"assistant\")\n",
    "        self.model_options = [\"brainstorm\", \"assistant\", \"director\", \"prompter\", \"researcher\", \"critic\"]\n",
    "        self.model_dropdown = ttk.OptionMenu(self.sidebar_frame, self.model_var, *self.model_options)\n",
    "        self.model_dropdown.pack()\n",
    "\n",
    "        self.workflow_label = ttk.Label(self.sidebar_frame, text=\"Workflow:\")\n",
    "        self.workflow_label.pack()\n",
    "        self.workflow_var = tk.StringVar(value=\"Optimator\")\n",
    "        self.workflow_options = [\"Optimator\"]\n",
    "        self.workflow_dropdown = ttk.OptionMenu(self.sidebar_frame, self.workflow_var, *self.workflow_options)\n",
    "        self.workflow_dropdown.pack()\n",
    "\n",
    "        self.iterations_label = ttk.Label(self.sidebar_frame, text=\"Iterations:\")\n",
    "        self.iterations_label.pack()\n",
    "        self.iterations_var = tk.IntVar(value=1)\n",
    "        self.iterations_options = list(range(1, 21))\n",
    "        self.iterations_dropdown = ttk.OptionMenu(self.sidebar_frame, self.iterations_var, 1, *self.iterations_options)\n",
    "        self.iterations_dropdown.pack()\n",
    "\n",
    "        self.modifier_tree = ModifierTreeView(self.sidebar_frame, selectmode=\"extended\")\n",
    "        self.modifier_tree.pack(fill=\"both\", expand=True)\n",
    "        self.populate_modifier_tree()\n",
    "\n",
    "        self.run_modifiers_button = ttk.Button(self.sidebar_frame, text=\"Run Modifier Groups\", command=self.run_modifier_groups)\n",
    "        self.run_modifiers_button.pack()\n",
    "\n",
    "        self.search_enabled = tk.BooleanVar(value=True)\n",
    "        self.search_checkbox = ttk.Checkbutton(self.sidebar_frame, text=\"Enable Search\", variable=self.search_enabled)\n",
    "        self.search_checkbox.pack()\n",
    "\n",
    "        self.improve_prompt = tk.BooleanVar(value=False)\n",
    "        self.improve_prompt_checkbox = ttk.Checkbutton(self.sidebar_frame, text=\"Automatically Improve Prompt\", variable=self.improve_prompt)\n",
    "        self.improve_prompt_checkbox.pack()\n",
    "\n",
    "        self.brainstorm_enabled = tk.BooleanVar(value=False)\n",
    "        self.brainstorm_checkbox = ttk.Checkbutton(self.sidebar_frame, text=\"Enable Brainstorm\", variable=self.brainstorm_enabled)\n",
    "        self.brainstorm_checkbox.pack()\n",
    "\n",
    "        self.run_button = ttk.Button(self.sidebar_frame, text=\"Run Workflow\", command=self.run_workflow)\n",
    "        self.run_button.pack()\n",
    "\n",
    "        self.single_model_button = ttk.Button(self.sidebar_frame, text=\"Run Single Model\", command=self.run_single_model)\n",
    "        self.single_model_button.pack()\n",
    "\n",
    "        self.manage_agents_button = ttk.Button(self.sidebar_frame, text=\"Manage Agents\", command=self.manage_agents)\n",
    "        self.manage_agents_button.pack()\n",
    "\n",
    "        self.agent_var = tk.StringVar(value=\"\")\n",
    "        self.agent_dropdown = ttk.OptionMenu(self.sidebar_frame, self.agent_var, \"\", *self.agent_manager.list_agents())\n",
    "        self.agent_dropdown.pack()\n",
    "\n",
    "    def setup_menu(self):\n",
    "        menubar = Menu(self)\n",
    "        filemenu = Menu(menubar, tearoff=0)\n",
    "        filemenu.add_command(label=\"Save Chat Log\", command=self.save_chat_log)\n",
    "        filemenu.add_separator()\n",
    "        filemenu.add_command(label=\"Exit\", command=self.quit)\n",
    "        menubar.add_cascade(label=\"File\", menu=filemenu)\n",
    "        self.config(menu=menubar)\n",
    "\n",
    "    def populate_modifier_tree(self):\n",
    "        # This method should be implemented to populate the modifier tree\n",
    "        # with the appropriate groups and modifiers\n",
    "        pass\n",
    "\n",
    "    def save_chat_log(self):\n",
    "        file_path = filedialog.asksaveasfilename(defaultextension=\".txt\", filetypes=[(\"Text Files\", \"*.txt\")])\n",
    "        if file_path:\n",
    "            try:\n",
    "                with open(file_path, \"w\") as file:\n",
    "                    file.write(self.chat_history.get(\"1.0\", \"end\"))\n",
    "                logger.info(f\"Chat log saved to {file_path}\")\n",
    "            except IOError as e:\n",
    "                logger.error(f\"Error saving chat log: {e}\")\n",
    "                messagebox.showerror(\"Error\", f\"Failed to save chat log: {e}\")\n",
    "\n",
    "    def run_modifier_groups(self):\n",
    "        selected_modifiers = self.modifier_tree.selected_modifiers\n",
    "        if not selected_modifiers:\n",
    "            return\n",
    "\n",
    "        user_input = self.user_input.get()\n",
    "        if not user_input and self.last_output:\n",
    "            user_input = self.last_output\n",
    "\n",
    "        if not user_input:\n",
    "            return\n",
    "\n",
    "        self.update_chat_history(\"User:\", user_input)\n",
    "        asyncio.create_task(self.process_modifier_groups(user_input, selected_modifiers))\n",
    "\n",
    "    async def process_modifier_groups(self, user_input: str, modifiers: List[str]):\n",
    "        for modifier in modifiers:\n",
    "            self.update_chat_history(\"Modifier:\", modifier, \"thinking\")\n",
    "            model_output = await self.model_manager.generate_response(\n",
    "                \"assistant\",\n",
    "                f\"{user_input}\\n\\nModifier: {modifier}\",\n",
    "                self.chat_log,\n",
    "                self.context,\n",
    "                self.search_manager\n",
    "            )\n",
    "            self.update_chat_history(\"Assistant:\", model_output)\n",
    "            user_input = model_output\n",
    "\n",
    "        self.last_output = user_input\n",
    "        self.modifier_tree.selected_modifiers = []\n",
    "\n",
    "    def manage_agents(self):\n",
    "        action = simpledialog.askstring(\n",
    "            \"Manage Agents\",\n",
    "            \"Enter 'create' to create a new agent or 'list' to see existing agents:\"\n",
    "        )\n",
    "        if action == \"create\":\n",
    "            self.create_agent()\n",
    "        elif action == \"list\":\n",
    "            self.list_agents()\n",
    "\n",
    "    def create_agent(self):\n",
    "        name = simpledialog.askstring(\"Create Agent\", \"Enter agent name:\")\n",
    "        if not name:\n",
    "            return\n",
    "        instruction = simpledialog.askstring(\"Create Agent\", \"Enter agent instruction:\")\n",
    "        if not instruction:\n",
    "            return\n",
    "        model_config = simpledialog.askstring(\"Create Agent\", \"Enter model configuration (JSON format):\")\n",
    "        try:\n",
    "            model_config = json.loads(model_config)\n",
    "        except json.JSONDecodeError:\n",
    "            messagebox.showerror(\"Error\", \"Invalid JSON for model configuration\")\n",
    "            return\n",
    "\n",
    "        if self.agent_manager.create_agent(name, instruction, model_config):\n",
    "            messagebox.showinfo(\"Success\", f\"Agent '{name}' created successfully\")\n",
    "            self.update_agent_dropdown()\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", f\"Agent '{name}' already exists\")\n",
    "\n",
    "    def list_agents(self):\n",
    "        agents = self.agent_manager.list_agents()\n",
    "        if agents:\n",
    "            messagebox.showinfo(\"Agents\", \"\\n\".join(agents))\n",
    "        else:\n",
    "            messagebox.showinfo(\"Agents\", \"No agents created yet\")\n",
    "\n",
    "    def update_agent_dropdown(self):\n",
    "        menu = self.agent_dropdown[\"menu\"]\n",
    "        menu.delete(0, \"end\")\n",
    "        for agent in self.agent_manager.list_agents():\n",
    "            menu.add_command(label=agent, command=partial(self.agent_var.set, agent))\n",
    "\n",
    "    def run_single_model(self):\n",
    "        user_input = self.user_input.get()\n",
    "        if not user_input:\n",
    "            return\n",
    "\n",
    "        selected_model = self.model_var.get()\n",
    "        selected_agent = self.agent_var.get()\n",
    "\n",
    "        self.update_chat_history(\"User:\", user_input)\n",
    "        self.context = generate_convo_context(user_input, self.chat_log)\n",
    "        asyncio.create_task(self.process_single_model(selected_model, user_input, selected_agent))\n",
    "\n",
    "    async def process_single_model(self, model_type: str, user_input: str, agent_name: Optional[str] = None):\n",
    "        self.update_chat_history(f\"{model_type.capitalize()}:\", \"Processing...\", \"thinking\")\n",
    "\n",
    "        if agent_name:\n",
    "            agent = self.agent_manager.get_agent(agent_name)\n",
    "            if agent:\n",
    "                model = ModelFactory.create_model(agent[\"instruction\"], **agent[\"model_config\"])\n",
    "                model_output = await model.generate_content(user_input)\n",
    "                model_output = model_output.text if model_output.text else \"\"\n",
    "            else:\n",
    "                model_output = \"Error: Agent not found\"\n",
    "        else:\n",
    "            model_output = await self.model_manager.generate_response(\n",
    "                model_type, user_input, self.chat_log, self.context, self.search_manager\n",
    "            )\n",
    "\n",
    "        self.update_chat_history(f\"{model_type.capitalize()}:\", model_output)\n",
    "        self.last_output = model_output\n",
    "\n",
    "    def run_workflow(self, event=None):\n",
    "        user_input = self.user_input.get()\n",
    "        self.user_input.delete(0, tk.END)\n",
    "\n",
    "        if not user_input and self.last_output:\n",
    "            user_input = self.last_output\n",
    "\n",
    "        if not user_input:\n",
    "            return\n",
    "\n",
    "        self.update_chat_history(\"User:\", user_input)\n",
    "        self.current_prompt = user_input\n",
    "        self.context = generate_convo_context(self.current_prompt, self.chat_log)\n",
    "\n",
    "        workflow = self.workflow_var.get()\n",
    "        asyncio.create_task(self.process_workflow(workflow, user_input))\n",
    "\n",
    "    async def process_workflow(self, workflow: str, user_input: str):\n",
    "        if workflow == \"Optimator\":\n",
    "            await self.run_optimator_workflow(user_input)\n",
    "        # Add more workflows here as needed\n",
    "\n",
    "    async def run_optimator_workflow(self, user_input: str):\n",
    "        if self.improve_prompt.get():\n",
    "            self.update_chat_history(\"Prompter:\", \"Improving prompt...\", \"thinking\")\n",
    "            improved_prompt = await self.model_manager.generate_response(\n",
    "                \"prompter\", user_input, self.chat_log, self.context, self.search_manager\n",
    "            )\n",
    "            user_input = improved_prompt\n",
    "            self.update_chat_history(\"Prompter:\", improved_prompt)\n",
    "\n",
    "        if self.brainstorm_enabled.get():\n",
    "            self.update_chat_history(\"Brainstorm:\", \"Generating ideas...\", \"thinking\")\n",
    "            brainstorm_output = await self.model_manager.generate_response(\n",
    "                \"brainstorm\", user_input, self.chat_log, self.context, self.search_manager\n",
    "            )\n",
    "            user_input += f\"\\n\\nBrainstorm ideas:\\n{brainstorm_output}\"\n",
    "            self.update_chat_history(\"Brainstorm:\", brainstorm_output)\n",
    "\n",
    "        iterations = 0\n",
    "        total_iterations = self.iterations_var.get()\n",
    "        while iterations < total_iterations:\n",
    "            self.update_chat_history(\"Assistant:\", \"Processing...\", \"thinking\")\n",
    "            assistant_output = await self.model_manager.generate_response(\n",
    "                \"assistant\", user_input, self.chat_log, self.context, self.search_manager\n",
    "            )\n",
    "            self.update_chat_history(\"Assistant:\", assistant_output)\n",
    "            self.last_output = assistant_output\n",
    "\n",
    "            self.update_chat_history(\"Critic:\", \"Evaluating...\", \"thinking\")\n",
    "            critic_output = await self.model_manager.generate_response(\n",
    "                \"critic\", assistant_output, self.chat_log, self.context, self.search_manager\n",
    "            )\n",
    "            self.update_chat_history(\"Critic:\", critic_output)\n",
    "\n",
    "            self.update_chat_history(\"Director:\", \"Planning next steps...\", \"thinking\")\n",
    "            director_output = await self.model_manager.generate_response(\n",
    "                \"director\", critic_output, self.chat_log, self.context, self.search_manager\n",
    "            )\n",
    "            self.update_chat_history(\"Director:\", director_output)\n",
    "            iterations += 1\n",
    "\n",
    "    def update_chat_history(self, speaker: str, message: str, message_type: str = \"normal\"):\n",
    "        self.chat_history.config(state=\"normal\")\n",
    "        if message_type == \"thinking\":\n",
    "            self.chat_history.insert(tk.END, f\"{speaker} \")\n",
    "        else:\n",
    "            self.chat_history.insert(tk.END, f\"{speaker} {message}\\n\\n\")\n",
    "        self.chat_history.config(state=\"disabled\")\n",
    "        self.chat_history.see(tk.END)\n",
    "        self.update()\n",
    "\n",
    "    def run(self):\n",
    "        self.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env\n",
    "GEMINI_API_KEY=\"AIzaSyAsHz1B6g-Ta5nxqszAu-wPahOP0x5Wfko\"\n",
    "XI_LABS_API_KEY=\"1bd2bf3116d18f3273f245efe322566a\"\n",
    "EDGE_DRIVER_PATH = \"C:/Users/Fuckmaster/edgedriver_win64/msedgedriver.exe\"\n",
    "SCRAPEOPS_API_KEY = \"c865c0ea-f92e-47d5-96bc-67ea1c568fd7\"\n",
    "PROXIES=socks4://200.80.227.234:4145,socks4://185.89.156.130:5678,http://45.173.12.141:1994,socks4://179.97.193.250:4153,socks4://222.212.85.149:5678,socks4://110.77.149.50:5678,socks4://184.181.217.210:4145\n",
    "\n",
    "GOOGLE_CUSTOM_SEARCH_ENGINE_ID=\"32e9bbeb5cbee467a\"\n",
    "GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY=\"AIzaSyA4rykIBGRoFTPjYAWoABlEyVs51K2geMo\"\n",
    "\n",
    "\n",
    "BRAVE_SEARCH_RATE_LIMIT=1 #PER SECOND\n",
    "BRAVE_SEARCH_MAX_REQUESTS=2000 #PER MONTH\n",
    "BRAVE_SEARCH_API_KEY=\"BSAYRSx_nlSSJQQ6vlNfDMByACSQpTa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AgentManager:\n",
    "    def __init__(self, file_path: str = \"agents.json\"):\n",
    "        self.file_path = file_path\n",
    "        self.agents = self.load_agents()\n",
    "\n",
    "    def load_agents(self) -> Dict[str, Dict[str, Any]]:\n",
    "        try:\n",
    "            if os.path.exists(self.file_path):\n",
    "                with open(self.file_path, \"r\") as f:\n",
    "                    return json.load(f)\n",
    "            return {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Error loading agents from {self.file_path}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def save_agents(self) -> None:\n",
    "        try:\n",
    "            with open(self.file_path, \"w\") as f:\n",
    "                json.dump(self.agents, f, indent=2)\n",
    "        except IOError as e:\n",
    "            logger.error(f\"Error saving agents to {self.file_path}: {e}\")\n",
    "\n",
    "    def create_agent(self, name: str, instruction: str, model_config: Dict[str, Any]) -> bool:\n",
    "        if name in self.agents:\n",
    "            logger.warning(f\"Agent '{name}' already exists\")\n",
    "            return False\n",
    "        self.agents[name] = {\"instruction\": instruction, \"model_config\": model_config}\n",
    "        self.save_agents()\n",
    "        return True\n",
    "\n",
    "    def get_agent(self, name: str) -> Optional[Dict[str, Any]]:\n",
    "        return self.agents.get(name)\n",
    "\n",
    "    def list_agents(self) -> list:\n",
    "        return list(self.agents.keys())\n",
    "\n",
    "    def update_agent(self, name: str, instruction: Optional[str] = None, model_config: Optional[Dict[str, Any]] = None) -> bool:\n",
    "        if name not in self.agents:\n",
    "            logger.warning(f\"Agent '{name}' does not exist\")\n",
    "            return False\n",
    "        if instruction is not None:\n",
    "            self.agents[name][\"instruction\"] = instruction\n",
    "        if model_config is not None:\n",
    "            self.agents[name][\"model_config\"] = model_config\n",
    "        self.save_agents()\n",
    "        return True\n",
    "\n",
    "    def delete_agent(self, name: str) -> bool:\n",
    "        if name not in self.agents:\n",
    "            logger.warning(f\"Agent '{name}' does not exist\")\n",
    "            return False\n",
    "        del self.agents[name]\n",
    "        self.save_agents()\n",
    "        return True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
