import langchain
from ollama import Client as OllamaClient
from duckduckgo_search import DDGS
from concurrent.futures import ThreadPoolExecutor

def web_search(query):
    """Perform a web search using DuckDuckGo and return results."""
    try:
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(DDGS().text, query, max_results=5)
            results = future.result()
        return results
    except Exception as e:
        print(f"Error during web search: {e}")
        return []

def integrate_search_results(response, search_query):
    """Integrate web search results into the response."""
    results = web_search(search_query)
    if not results:
        return response

    result_text = "\n\nWeb Search Results:\n"
    for i, result in enumerate(results, 1):
        result_text += f"{i}. {result['title']}\n{result['href']}\n{result['body']}\n\n"
    return response + result_text

class OllamaModel:
    def __init__(self, model_name):
        self.name = model_name
        self.client = OllamaClient()

    def generate(self, prompt):
        response = self.client.generate(model=self.name, prompt=prompt)
        return response['response']

assistant_models = {
    "Coder": OllamaModel("codellama"),
    "Writer": OllamaModel("llama2"),
    "Brainstormer": OllamaModel("llama2"),
    "Critic": OllamaModel("llama2"),
    "Coordinator": OllamaModel("llama2")
}

system_prompts = {
    "Coder": "You are a code generation assistant. Help in generating code snippets, debugging issues, and optimizing existing code.",
    "Writer": "You are a creative writing assistant. Generate creative content such as stories, articles, emails, etc.",
    "Brainstormer": "You are an idea generator. Suggest innovative ideas, generate brainstorming sessions, and provide creative solutions.",
    "Critic": "You are a quality assessment assistant. Provide feedback on the quality of generated content, suggesting improvements.",
    "Coordinator": "You are a coordinator model responsible for managing tasks and delegating them to other assistant models."
}

def detect_web_search(text):
    """Detect if the text contains a request for web search."""
    return "WEB_SEARCH:" in text

def extract_query_from_web_search(text):
    """Extract the query from the WEB_SEARCH command."""
    if "WEB_SEARCH:" in text:
        return text.split("WEB_SEARCH:")[1].strip().split("|")[0].strip()
    return None

def requires_more_info(text):
    """Check if the response requires more information."""
    uncertain_words = ["don't know", "not sure", "more context needed"]
    return any(word in text.lower() for word in uncertain_words)

def delegate_task(current_state):
    """Delegate a task to an appropriate assistant model."""
    prompt = f"{system_prompts['Coordinator']}\n\nCurrent task: {current_state}\n\nWhich model should handle this task?"
    response = assistant_models["Coordinator"].generate(prompt)
    return response.strip()

def process_task(model_type, task):
    """Process the task using the appropriate model."""
    prompt = f"{system_prompts[model_type]}\n\nTask: {task}"
    response = assistant_models[model_type].generate(prompt)
    
    if detect_web_search(response):
        query = extract_query_from_web_search(response)
        search_results = web_search(query)
        response += f"\n\nWeb search results for '{query}':\n" + "\n".join(str(result) for result in search_results)
    
    return response

def iterative_conversation_loop(current_state):
    while True:
        model_type_to_use = delegate_task(current_state)
        
        if model_type_to_use in assistant_models:
            response = process_task(model_type_to_use, current_state)
            print(f"{model_type_to_use}: {response}")
        else:
            print(f"Unknown model type: {model_type_to_use}")
        
        next_input = input("User: ")
        if next_input.lower() in ["exit", "quit"]:
            break
        current_state = next_input

# Initial state of the conversation
initial_state = "Please provide a task."
iterative_conversation_loop(initial_state)
