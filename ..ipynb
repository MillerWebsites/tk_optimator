{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8daeb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Gemini Pro 1.py\n",
    "\n",
    "# --- Gemini Pro 1.5 latest base class ---\n",
    "\n",
    "# Import Google-GenerativeAI and set up a conversational chat with Gemini Pro 1.5 latest\n",
    "from google_generativeai import GoogleGenerativeAI\n",
    "\n",
    "client = GoogleGenerativeAI()    # Initialize the client with your API key\n",
    "\n",
    "# Start the conversation\n",
    "client.converse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd7d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_providers.py\n",
    "\n",
    "from os import system\n",
    "import openai  # Assuming OpenAI for now, add others as needed\n",
    "from abc import ABC, abstractmethod\n",
    "import google.generativeai as genai\n",
    "from config import GEMINI_API_KEY, SAFETY_SETTINGS\n",
    "import json\n",
    "\n",
    "\n",
    "# genai.GenerativeModel(model_name: str = \"gemini-1.5-pro-latest\", safety_settings: Any | None = None, generation_config: GenerationConfigType | None = None, tools: FunctionLibraryType | None = None, tool_config: ToolConfigType | None = None, system_instruction: ContentType | None = None) -> GenerativeModel\n",
    "\n",
    "\n",
    "\n",
    "class LLMProvider(ABC):\n",
    "    @abstractmethod\n",
    "    def generate_response(self, prompt, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "#load agents from json file and return as dictionary\n",
    "def load_agents():\n",
    "    with open(\"agents.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_agents(agents):\n",
    "    with open(\"agents.json\", \"w\") as f:\n",
    "        json.dump(agents, f, indent=2)\n",
    "\n",
    "class OpenAIProvider(LLMProvider):\n",
    "    def __init__(self, api_key):\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    def generate_response(self, prompt, **kwargs):\n",
    "        response = openai.Completion.create(\n",
    "            engine=kwargs.get(\"engine\", \"text-davinci-003\"),  # Default engine\n",
    "            prompt=prompt,\n",
    "            max_tokens=kwargs.get(\"max_tokens\", 100),  # Default max tokens\n",
    "            temperature=kwargs.get(\"temperature\", 0.7),  # Default temperature\n",
    "            # ... other OpenAI parameters\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "\n",
    "\n",
    "class GeminiProvider(LLMProvider):\n",
    "    def __init__(self, api_key, **kwargs):\n",
    "        self.api_key = api_key\n",
    "        self.agents = load_agents()\n",
    "        default_assistant = \"default_assistant\"\n",
    "        genai.configure(api_key=self.api_key)\n",
    "        self.client = genai.GenerativeModel(\n",
    "            model_name=\"models/gemini-1.5-pro-latest\", \n",
    "            safety_settings=SAFETY_SETTINGS, \n",
    "            generation_config={\"temperature\": 0.5,},\n",
    "            system_instruction=self.agents[default_assistant]\n",
    "            tools = agents.default_tools\n",
    "            )\n",
    "        \n",
    "\n",
    "    def generate_response(self, prompt, **kwargs):\n",
    "        response = self.client.generate_content(prompt, **kwargs)\n",
    "        return response.text if response.text else \"\""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
