Artificial Tweetening - the bad taste you get in your mouth from pervasive yet insidious bot armies recently AI-enhanced, leading to an increasing difficulty to near impossibility to detect and automatically disable them with any software the world currently possesses. 
PsuedoFed - not a drug; an agent of the state that does not follow the law (as described in the TwitterFiles)
Countertwitters (pronounced like counterfeiters) - The name for any bots or people that attempt to create fake social currency, legitimacy, or popularity by using multiple accounts that have fake conversations amongst themselves in the comments, typically content from actually popular people. 
Tweetastrophe - A discourse that has devolved into what is most aptly described as a verbal defecate- throwing match, often not long after it becomes clear that the discussion topic is too complex to fit into 280 characters, resulting in one or both parties adopting tactics more typically associated with trolls or kids throwing a tantrum to get their way or feel like they won regardless of actually doing so

**Concept:** This song would explore the idea that everyone, no matter how ordinary they may seem, has a rich inner world full of thoughts, dreams, and fears. The lyrics would use playful imagery and metaphors to delve into the hidden depths of the human experience, questioning the nature of reality and the masks we wear to navigate it. The seemingly nonsensical image of purple penguins pondering polka dots would serve as a springboard to explore the absurdity of existence and the unique perspectives each individual holds. The binary code whispers of the universe could represent the underlying order and complexity of life, while the costumes of fear would highlight the vulnerability and insecurity we all share.

C:\Users\Fuckmaster\Downloads\imagine-a-breathtaking-mural-a-true-masterpiece-th-fRiMHlfYRF6QuOk4no1lxw-kPm4JMHdROG-rbWyELVdjQ_upscayl_2x_ultramix_balanced.jpg

I'm working on creating a python program that begins with a user input to an LLM (using google-generativeai's gemini-pro), then after the ai's response, the program will prompt the user to select a modifier group.  There will be several modifier groups, and the number of them should be easy to modify if I want to add an extra modifier group for example.  Once a modifier group is selected, the program will display all of the modifiers in that group and ask the user to select one of them to apply.  When a modifier is selected, it is automatically appended to the last three pairs of messages from the conversation's history (6 messages total, 3 from the user and 3 ai responses) and then it is fed back as a new prompt / input to the LLM to get a response that newly incorporates whatever the modifier says to do.  This process of applying modifiers and generating new responses is a loop in which the user can keep selecting modifiers until satisfied, or they can select AUTOCOMPLETE which would apply all of modifiers in the modifier group one by one, sequentially, and display each ai response as it progresses through the modifier group.  Here is my script so far.  Can you make a GUI for it, or is there other bugs or issues that need to be addressed first?

******CURRENT DRAFT OF CODE******
import time
import google.generativeai as genai
import os
from datetime import date

# Constants
API_KEY = "AIzaSyAsHz1B6g-Ta5nxqszAu-wPahOP0x5Wfko"
MAX_CHAT_HISTORY_LENGTH = 30

# Begin Groups of Modifiers, grouped by type or style of task and content being worked on.
# On run, modifiers are appended to the end of the latest ai response and fed back to the AI to either critique or refine its previous response.

# Modifiers for the 'Creative Spark' task


MODIFIERS_1 = [
    " generate a new revision with a focus on enriching the idea with vivid details or sensory descriptions.",
]

MODIFIERS_2 = [
    "STREAM OF CONSCIOUSNESS BRAINSTORM: Initiate with a session to generate 10 creative ideas related to the initial prompt, emphasizing unconventional and diverse concepts to foster novelty and innovation.",
    "CREATE FIRST DRAFT OUTPUT: Utilize the ideas from the brainstorming session to compose a first draft in response to the initial prompt, integrating the most promising concepts in a coherent and original way.",
    "CRITIQUE REPONSE, OPTIMIZE PROMPT: Critique the first draft output with respect to the initial prompt, identifying all areas with room for further development and improvement. Use this analysis to develop a new and improved, optimized version of the prompt.  Only respond with this new prompt.",
]

CREATIVE_SPARK = [
]

MODIFIERS_4 = [
    ]

generation_config = {
    "temperature": 0.9,
    "top_p": 0.8,
    "top_k": 5,
    "max_output_tokens": 16000,
}

safety_settings = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
]

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel("gemini-1.5-pro-latest", generation_config=generation_config, safety_settings=safety_settings)

def append_to_history(chat_history, user_input, ai_response):
    user_message = f"User: {user_input}"
    ai_message = f"Assistant: {ai_response}"
    chat_history.append((user_message, ai_message))
    if len(chat_history) > MAX_CHAT_HISTORY_LENGTH:
        chat_history.pop(0)

def display_history(chat_history):
    print("===== Refinement History =====")
    for i, (user_input, ai_response) in enumerate(chat_history):
        print(f"Step {i+1}:")
        print(f"  {user_input}")
        print(f"  {ai_response}\n")

def save_chat_history(chat_history, filename="chat_history.txt"):
    try:
        with open(filename, "w") as file:
            for user_input, ai_response in chat_history:
                file.write(f"{user_input}\n{ai_response}\n")
    except IOError as e:
        print(f"IOError occurred while saving chat history: {e}")
    except Exception as e:
        print(f"An unexpected error occurred while saving chat history: {e}")

def select_modifier_group():
    while True:
        try:
            user_choice = int(input("Select a modifier group (1-4) or type 5 to finish: "))
            if 1 <= user_choice <= 5:
                return user_choice
            print("Invalid input. Please enter a number between 1 and 5.")
        except ValueError:
            print("Invalid input. Please enter a number between 1 and 5.")


def generate_content_with_modifiers(initial_prompt, chat_history, user_choice):
    current_prompt = initial_prompt
    modifiers = {1: MODIFIERS_1, 2: MODIFIERS_2, 3: CREATIVE_SPARK, 4: MODIFIERS_4}
    modifiers_to_use = list(modifiers[user_choice])

    automatic_finish = False  # Flag to track if the user selects automatic completion
    applied_modifiers = set()  # Set to track applied modifiers in automatic mode

    # Initial AI response
    response = model.generate_content("User: " + current_prompt)
    ai_response = response.text
    chat_history.append(("User: " + current_prompt, "AI: " + ai_response))
    print(f"AI Response: {ai_response}")

    while True:
        if not automatic_finish:
            print(f"\nRefinement Step: {len(chat_history)}")
            print("Enter the number of the modifier you choose or type 'f' to provide custom feedback for the last output:")
            for i, modifier in enumerate(modifiers_to_use, start=1):
                print(f"{i}. Apply modifier: {modifier}")
            print(f"{len(modifiers_to_use) + 1}. Finish automatically")
            print(f"{len(modifiers_to_use) + 2}. End the refinement process")
            user_input = input("Your choice: ")
            if user_input.lower() == 'f':  # Provide custom feedback
                custom_feedback = input("Enter your custom feedback for the last output: ")
                current_prompt = custom_feedback  # Update current_prompt with the new custom feedback
                apply_modifier_and_generate_response(custom_feedback, chat_history, current_prompt)
                continue

            try:
                user_choice = int(user_input)
                if user_choice == len(modifiers_to_use) + 1:
                    automatic_finish = True
                    continue
                elif user_choice == len(modifiers_to_use) + 2:
                    break
                else:
                    modifier = modifiers_to_use[user_choice - 1]
                    current_prompt = apply_modifier_and_generate_response(modifier, chat_history, current_prompt)
            except ValueError:
                print("Invalid input. Please enter a number.")
        else:
            for modifier in modifiers_to_use:
                if modifier not in applied_modifiers:
                    apply_modifier_and_generate_response(modifier, chat_history, current_prompt)
                    applied_modifiers.add(modifier)
            break  # Exit after all modifiers have been applied once

    return chat_history

def apply_modifier_and_generate_response(modifier, chat_history, current_prompt):
    # Build the conversation context for the model by concatenating past exchanges
    combined_prompt = "\n".join([f"User: {mod}\nAssistant: {resp}" for mod, resp in chat_history])
    combined_prompt += f"\nUser: {modifier}\nAssistant: "  # Append the new user input

    # Generate content based on the full conversation context
    response = model.generate_content(combined_prompt)
    ai_response = response.text

    # Append both the user input and AI response to the chat history
    chat_history.append((modifier, ai_response))

    print(f"Modifier Applied: {modifier}")
    print(f"AI Response: {ai_response}")
    time.sleep(1)  # Simulate processing delay

    return chat_history  # Return updated chat history if needed for clarity, though not necessary with list modifications

def main():
    try:
        initial_prompt = input("Enter your initial prompt: ").strip()
        if not initial_prompt:
            print("Initial prompt cannot be empty. Please provide a valid input.")
            return
        user_choice = select_modifier_group()
    
        if user_choice:
            chat_history = []  # Initialize chat history here
            print("Generating chat history...")
            chat_history = generate_content_with_modifiers(initial_prompt, chat_history, user_choice)
            print("Chat history generated. Displaying now...")
            display_history(chat_history)
    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    main()



307572 


Thumbs down robots are all on this.  The "thumbs down" is basically now an indicator of what the machine is trying to hide.


import numpy as np  # For numerical computations and array operations
import tensorflow as tf  # For neural network implementation
import chaospy as cp  # To explore other chaotic maps and analysis tools

def logistic_map(x, r):
    return r * x * (1 - x)

def generate_chaos_factor(num_iterations, initial_value, r):
    chaos_sequence = np.zeros(num_iterations)
    chaos_sequence[0] = initial_value
    for i in range(1, num_iterations):
        chaos_sequence[i] = logistic_map(chaos_sequence[i-1], r)
    return chaos_sequence

chaos_factor = generate_chaos_factor(num_training_steps, 0.4, 3.8)  # Example values
learning_rate = base_learning_rate * chaos_factor  # Scale or adjust using chaos
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

chaos_noise = np.random.normal(scale=chaos_factor)  # Generate noise based on chaos
model.layers[i].kernel = model.layers[i].kernel + chaos_noise  # Example weight perturbation


model.compile(optimizer=optimizer, loss='...', metrics=['...'])
model.fit(x_train, y_train, epochs=..., validation_data=(x_val, y_val))